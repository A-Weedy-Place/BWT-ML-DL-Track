{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Understanding RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What are Recurrent Neural Networks, and how do they differ from traditional feedforward neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs are a type of neural network designed for sequence data (like time series or text). They remember past information to make decisions.\n",
    "\n",
    "\n",
    "Difference from Feedforward Networks: \\\n",
    "Traditional neural networks (feedforward) just process inputs one by one without memory. RNNs have loops, allowing them to keep information from previous inputs. (Recurrusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Explain the working of RNN, and how information is passed through the network over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At each step, RNN takes an input and combines it with the hidden state (memory of the past). \n",
    "- This combination is passed through a layer to produce the output and update the hidden state. \n",
    "- The updated hidden state is passed on to the next step, so NN remembers previoous inputs. \\\n",
    "\n",
    "info is passed by:\n",
    "- The hidden state carries the memory of what has been seen so far.\n",
    "- As new inputs come in, the hidden state gets updated, so the network is \"learning\" as it processes the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking RNN Layers and Bi-directional Architecture\n",
    "\n",
    "**Stacking RNN Layers:**\n",
    "- **Advantages:** \n",
    "  - **Complex Patterns:** Multiple RNN layers can capture more complex patterns in the data.\n",
    "  - **Higher Capacity:** They have a higher capacity to learn from long sequences.\n",
    "- **Drawbacks:**\n",
    "  - **Vanishing/Exploding Gradients:** More layers can lead to vanishing or exploding gradients, making training harder.\n",
    "  - **Computational Cost:** More layers increase computation and memory usage.\n",
    "\n",
    "**Bi-directional RNNs:**\n",
    "- **Definition:** Bi-directional RNNs process data in both forward and backward directions.\n",
    "- **Enhancement:** This allows the model to learn from both past and future context in sequences, improving performance on tasks where context from both directions is important.\n",
    "\n",
    "### Hybrid Architecture\n",
    "\n",
    "**Hybrid Architecture:**\n",
    "- **Definition:** Combining RNNs with other models (e.g., CNNs, attention mechanisms) to leverage their strengths.\n",
    "- **Examples:**\n",
    "  - **CNN + RNN:** CNNs can extract features from sequences (like text or images), and RNNs can model temporal dependencies in those features.\n",
    "  - **Attention + RNN:** Attention mechanisms help RNNs focus on relevant parts of the input sequence, improving performance on tasks like machine translation.\n",
    "\n",
    "### Types of RNNs\n",
    "\n",
    "1. **Vanilla RNN:**\n",
    "   - **Structure:** Basic RNN with simple connections between layers.\n",
    "   - **Differences:** Struggles with long-term dependencies due to vanishing gradients.\n",
    "\n",
    "2. **Long Short-Term Memory (LSTM):**\n",
    "   - **Structure:** Includes gates (input, output, forget) to control the flow of information.\n",
    "   - **Differences:** Better at capturing long-term dependencies compared to vanilla RNNs.\n",
    "\n",
    "3. **Gated Recurrent Unit (GRU):**\n",
    "   - **Structure:** Similar to LSTMs but with fewer gates (update and reset gates).\n",
    "   - **Differences:** Often simpler and faster than LSTMs with comparable performance.\n",
    "\n",
    "4. **Bidirectional RNN:**\n",
    "   - **Structure:** Processes input in both forward and backward directions.\n",
    "   - **Differences:** Captures context from both ends of the sequence, enhancing performance on tasks requiring understanding from both directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweet sentiment Analysis dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PMLS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\PMLS\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import emoji\n",
    "nltk.download('punkt')\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from keras.layers import LSTM, Dense, SimpleRNN, Embedding, Flatten, Dropout\n",
    "from keras.activations import softmax\n",
    "from sklearn.model_selection import train_test_split\n",
    "# ignore warnings   \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>message to examine</th>\n",
       "      <th>label (depression result)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>106</td>\n",
       "      <td>just had a real good moment. i missssssssss hi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>217</td>\n",
       "      <td>is reading manga  http://plurk.com/p/mzp1e</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>220</td>\n",
       "      <td>@comeagainjen http://twitpic.com/2y2lx - http:...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>288</td>\n",
       "      <td>@lapcat Need to send 'em to my accountant tomo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>540</td>\n",
       "      <td>ADD ME ON MYSPACE!!!  myspace.com/LookThunder</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index                                 message to examine  \\\n",
       "0    106  just had a real good moment. i missssssssss hi...   \n",
       "1    217         is reading manga  http://plurk.com/p/mzp1e   \n",
       "2    220  @comeagainjen http://twitpic.com/2y2lx - http:...   \n",
       "3    288  @lapcat Need to send 'em to my accountant tomo...   \n",
       "4    540      ADD ME ON MYSPACE!!!  myspace.com/LookThunder   \n",
       "\n",
       "   label (depression result)  \n",
       "0                          0  \n",
       "1                          0  \n",
       "2                          0  \n",
       "3                          0  \n",
       "4                          0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('sentiment_tweets3.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>106</td>\n",
       "      <td>just had a real good moment. i missssssssss hi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>217</td>\n",
       "      <td>is reading manga  http://plurk.com/p/mzp1e</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>220</td>\n",
       "      <td>@comeagainjen http://twitpic.com/2y2lx - http:...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>288</td>\n",
       "      <td>@lapcat Need to send 'em to my accountant tomo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>540</td>\n",
       "      <td>ADD ME ON MYSPACE!!!  myspace.com/LookThunder</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index                                               Text  Label\n",
       "0    106  just had a real good moment. i missssssssss hi...      0\n",
       "1    217         is reading manga  http://plurk.com/p/mzp1e      0\n",
       "2    220  @comeagainjen http://twitpic.com/2y2lx - http:...      0\n",
       "3    288  @lapcat Need to send 'em to my accountant tomo...      0\n",
       "4    540      ADD ME ON MYSPACE!!!  myspace.com/LookThunder      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rename(columns={'message to examine': 'Text', 'label (depression result)': 'Label'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'] = df['Text'].str.lower()\n",
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "# Remove HTML tags from 'Text' column\n",
    "df['Text'] = df['Text'].apply(remove_html_tags)\n",
    "\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "# Apply the function to the 'Text' column\n",
    "df['Text'] = df['Text'].apply(remove_urls)\n",
    "\n",
    "string.punctuation\n",
    "\n",
    "# Define the punctuation characters to remove\n",
    "punctuation = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', punctuation))\n",
    "\n",
    "# Apply remove_punctuation function to 'Text' column\n",
    "df['Text'] = df['Text'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cpy pasting this common slangs abbreviation list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words = {\n",
    "    \"AFAIK\": \"As Far As I Know\",\n",
    "    \"AFK\": \"Away From Keyboard\",\n",
    "    \"ASAP\": \"As Soon As Possible\",\n",
    "    \"ATK\": \"At The Keyboard\",\n",
    "    \"ATM\": \"At The Moment\",\n",
    "    \"A3\": \"Anytime, Anywhere, Anyplace\",\n",
    "    \"BAK\": \"Back At Keyboard\",\n",
    "    \"BBL\": \"Be Back Later\",\n",
    "    \"BBS\": \"Be Back Soon\",\n",
    "    \"BFN\": \"Bye For Now\",\n",
    "    \"B4N\": \"Bye For Now\",\n",
    "    \"BRB\": \"Be Right Back\",\n",
    "    \"BRT\": \"Be Right There\",\n",
    "    \"BTW\": \"By The Way\",\n",
    "    \"B4\": \"Before\",\n",
    "    \"B4N\": \"Bye For Now\",\n",
    "    \"CU\": \"See You\",\n",
    "    \"CUL8R\": \"See You Later\",\n",
    "    \"CYA\": \"See You\",\n",
    "    \"FAQ\": \"Frequently Asked Questions\",\n",
    "    \"FC\": \"Fingers Crossed\",\n",
    "    \"FWIW\": \"For What It's Worth\",\n",
    "    \"FYI\": \"For Your Information\",\n",
    "    \"GAL\": \"Get A Life\",\n",
    "    \"GG\": \"Good Game\",\n",
    "    \"GN\": \"Good Night\",\n",
    "    \"GMTA\": \"Great Minds Think Alike\",\n",
    "    \"GR8\": \"Great!\",\n",
    "    \"G9\": \"Genius\",\n",
    "    \"IC\": \"I See\",\n",
    "    \"ICQ\": \"I Seek you (also a chat program)\",\n",
    "    \"ILU\": \"ILU: I Love You\",\n",
    "    \"IMHO\": \"In My Honest/Humble Opinion\",\n",
    "    \"IMO\": \"In My Opinion\",\n",
    "    \"IOW\": \"In Other Words\",\n",
    "    \"IRL\": \"In Real Life\",\n",
    "    \"KISS\": \"Keep It Simple, Stupid\",\n",
    "    \"LDR\": \"Long Distance Relationship\",\n",
    "    \"LMAO\": \"Laugh My A.. Off\",\n",
    "    \"LOL\": \"Laughing Out Loud\",\n",
    "    \"LTNS\": \"Long Time No See\",\n",
    "    \"L8R\": \"Later\",\n",
    "    \"MTE\": \"My Thoughts Exactly\",\n",
    "    \"M8\": \"Mate\",\n",
    "    \"NRN\": \"No Reply Necessary\",\n",
    "    \"OIC\": \"Oh I See\",\n",
    "    \"PITA\": \"Pain In The A..\",\n",
    "    \"PRT\": \"Party\",\n",
    "    \"PRW\": \"Parents Are Watching\",\n",
    "    \"QPSA?\": \"Que Pasa?\",\n",
    "    \"ROFL\": \"Rolling On The Floor Laughing\",\n",
    "    \"ROFLOL\": \"Rolling On The Floor Laughing Out Loud\",\n",
    "    \"ROTFLMAO\": \"Rolling On The Floor Laughing My A.. Off\",\n",
    "    \"SK8\": \"Skate\",\n",
    "    \"STATS\": \"Your sex and age\",\n",
    "    \"ASL\": \"Age, Sex, Location\",\n",
    "    \"THX\": \"Thank You\",\n",
    "    \"TTFN\": \"Ta-Ta For Now!\",\n",
    "    \"TTYL\": \"Talk To You Later\",\n",
    "    \"U\": \"You\",\n",
    "    \"U2\": \"You Too\",\n",
    "    \"U4E\": \"Yours For Ever\",\n",
    "    \"WB\": \"Welcome Back\",\n",
    "    \"WTF\": \"What The F...\",\n",
    "    \"WTG\": \"Way To Go!\",\n",
    "    \"WUF\": \"Where Are You From?\",\n",
    "    \"W8\": \"Wait...\",\n",
    "    \"7K\": \"Sick:-D Laugher\",\n",
    "    \"TFW\": \"That feeling when\",\n",
    "    \"MFW\": \"My face when\",\n",
    "    \"MRW\": \"My reaction when\",\n",
    "    \"IFYP\": \"I feel your pain\",\n",
    "    \"TNTL\": \"Trying not to laugh\",\n",
    "    \"JK\": \"Just kidding\",\n",
    "    \"IDC\": \"I don't care\",\n",
    "    \"ILY\": \"I love you\",\n",
    "    \"IMU\": \"I miss you\",\n",
    "    \"ADIH\": \"Another day in hell\",\n",
    "    \"ZZZ\": \"Sleeping, bored, tired\",\n",
    "    \"WYWH\": \"Wish you were here\",\n",
    "    \"TIME\": \"Tears in my eyes\",\n",
    "    \"BAE\": \"Before anyone else\",\n",
    "    \"FIMH\": \"Forever in my heart\",\n",
    "    \"BSAAW\": \"Big smile and a wink\",\n",
    "    \"BWL\": \"Bursting with laughter\",\n",
    "    \"BFF\": \"Best friends forever\",\n",
    "    \"CSL\": \"Can't stop laughing\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_chat_words(text):\n",
    "    words = text.split()\n",
    "    for i, word in enumerate(words):\n",
    "        if word.lower() in chat_words:\n",
    "            words[i] = chat_words[word.lower()]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply replace_chat_words function to 'Text' column\n",
    "df['Text'] = df['Text'].apply(replace_chat_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PMLS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "# Get English stopwords from NLTK\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove stop words from text\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply remove_stopwords function to 'Text' column\n",
    "df['Text'] = df['Text'].apply(remove_stopwords)\n",
    "\n",
    "def remove_emojis(text):\n",
    "    return emoji.demojize(text)\n",
    "\n",
    "# Apply remove_emojis function to 'Text' column\n",
    "df['Text'] = df['Text'].apply(remove_emojis)\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Apply \n",
    "df['Text_lemmatized'] = df['Text'].apply(lambda x: ' '.join([wordnet_lemmatizer.lemmatize(word , pos='v') for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Text']\n",
    "y = df['Label']\n",
    "\n",
    "# Train Test Split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10314"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(oov_token = 'nothing')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "tokenizer.fit_on_texts(X_test)\n",
    "\n",
    "tokenizer.document_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length (maxlen): 75\n"
     ]
    }
   ],
   "source": [
    "maxlen = max(len(tokens) for tokens in X_train_sequences)\n",
    "print(\"Maximum sequence length (maxlen):\", maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=maxlen, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.7506 - loss: 0.5743 - val_accuracy: 0.7712 - val_loss: 0.5173\n",
      "Epoch 2/5\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 34ms/step - accuracy: 0.7597 - loss: 0.5615 - val_accuracy: 0.7824 - val_loss: 0.5310\n",
      "Epoch 3/5\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.7694 - loss: 0.5537 - val_accuracy: 0.7824 - val_loss: 0.5260\n",
      "Epoch 4/5\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.7647 - loss: 0.5546 - val_accuracy: 0.7824 - val_loss: 0.5367\n",
      "Epoch 5/5\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.7734 - loss: 0.5491 - val_accuracy: 0.7824 - val_loss: 0.5334\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.7873 - loss: 0.5284\n",
      "Test Accuracy (Simple RNN): 0.7824\n",
      "Epoch 1/5\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 48ms/step - accuracy: 0.6940 - loss: 0.6185 - val_accuracy: 0.7824 - val_loss: 0.5507\n",
      "Epoch 2/5\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 48ms/step - accuracy: 0.7749 - loss: 0.5514 - val_accuracy: 0.7824 - val_loss: 0.5565\n",
      "Epoch 3/5\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 62ms/step - accuracy: 0.7797 - loss: 0.5371 - val_accuracy: 0.7824 - val_loss: 0.5294\n",
      "Epoch 4/5\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 77ms/step - accuracy: 0.7747 - loss: 0.5373 - val_accuracy: 0.7824 - val_loss: 0.5240\n",
      "Epoch 5/5\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 62ms/step - accuracy: 0.7729 - loss: 0.5425 - val_accuracy: 0.7824 - val_loss: 0.5240\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.7873 - loss: 0.5176\n",
      "Test Accuracy (Stacked RNN): 0.7824\n",
      "Epoch 1/5\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 88ms/step - accuracy: 0.7415 - loss: 0.5845 - val_accuracy: 0.7950 - val_loss: 0.5039\n",
      "Epoch 2/5\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 77ms/step - accuracy: 0.7827 - loss: 0.5265 - val_accuracy: 0.8008 - val_loss: 0.5061\n",
      "Epoch 3/5\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 81ms/step - accuracy: 0.7813 - loss: 0.5186 - val_accuracy: 0.7857 - val_loss: 0.5124\n",
      "Epoch 4/5\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 78ms/step - accuracy: 0.7737 - loss: 0.5538 - val_accuracy: 0.8037 - val_loss: 0.5062\n",
      "Epoch 5/5\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 79ms/step - accuracy: 0.7807 - loss: 0.5235 - val_accuracy: 0.8032 - val_loss: 0.4797\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.8088 - loss: 0.4738\n",
      "Test Accuracy (Bidirectional RNN): 0.8032\n",
      "\n",
      "Model Comparison:\n",
      "Simple RNN - Test Accuracy: 0.7824\n",
      "Stacked RNN - Test Accuracy: 0.7824\n",
      "Bidirectional RNN - Test Accuracy: 0.8032\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Bidirectional, Dropout, Dense\n",
    "from tensorflow.keras.callbacks import History\n",
    "\n",
    "# Assuming X_train_padded, X_test_padded, y_train, y_test are already defined and preprocessed\n",
    "\n",
    "# Define Simple RNN Model\n",
    "model_simple_rnn = Sequential([\n",
    "    SimpleRNN(128, input_shape=(75, 1), return_sequences=True),\n",
    "    Dropout(0.5),\n",
    "    SimpleRNN(128),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile Simple RNN Model\n",
    "model_simple_rnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history_simple_rnn = model_simple_rnn.fit(X_train_padded, y_train, epochs=5, batch_size=32, validation_data=(X_test_padded, y_test))\n",
    "test_loss_simple_rnn, test_acc_simple_rnn = model_simple_rnn.evaluate(X_test_padded, y_test)\n",
    "print(f\"Test Accuracy (Simple RNN): {test_acc_simple_rnn:.4f}\")\n",
    "\n",
    "# Define Stacked RNN Model\n",
    "model_stacked_rnn = Sequential([\n",
    "    SimpleRNN(128, return_sequences=True, input_shape=(75, 1)),\n",
    "    Dropout(0.5),\n",
    "    SimpleRNN(128, return_sequences=True),\n",
    "    Dropout(0.5),\n",
    "    SimpleRNN(128),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile Stacked RNN Model\n",
    "model_stacked_rnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history_stacked_rnn = model_stacked_rnn.fit(X_train_padded, y_train, epochs=5, batch_size=32, validation_data=(X_test_padded, y_test))\n",
    "test_loss_stacked_rnn, test_acc_stacked_rnn = model_stacked_rnn.evaluate(X_test_padded, y_test)\n",
    "print(f\"Test Accuracy (Stacked RNN): {test_acc_stacked_rnn:.4f}\")\n",
    "\n",
    "# Define Bidirectional RNN Model\n",
    "model_bidirectional_rnn = Sequential([\n",
    "    Bidirectional(SimpleRNN(128, return_sequences=True), input_shape=(75, 1)),\n",
    "    Dropout(0.5),\n",
    "    Bidirectional(SimpleRNN(128, return_sequences=True)),\n",
    "    Dropout(0.5),\n",
    "    Bidirectional(SimpleRNN(128)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile Bidirectional RNN Model\n",
    "model_bidirectional_rnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history_bidirectional_rnn = model_bidirectional_rnn.fit(X_train_padded, y_train, epochs=5, batch_size=32, validation_data=(X_test_padded, y_test))\n",
    "test_loss_bidirectional_rnn, test_acc_bidirectional_rnn = model_bidirectional_rnn.evaluate(X_test_padded, y_test)\n",
    "print(f\"Test Accuracy (Bidirectional RNN): {test_acc_bidirectional_rnn:.4f}\")\n",
    "\n",
    "# Comparison of Results\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(f\"Simple RNN - Test Accuracy: {test_acc_simple_rnn:.4f}\")\n",
    "print(f\"Stacked RNN - Test Accuracy: {test_acc_stacked_rnn:.4f}\")\n",
    "print(f\"Bidirectional RNN - Test Accuracy: {test_acc_bidirectional_rnn:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
